{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75f4f51b-e00d-4fc0-8a22-654c454e9428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds6237/.conda/envs/torch-env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to import Triton-based MLP: No module named 'liger_kernel'. Falling back to vanilla SwiGLU MLP instead.\n",
      "Unable to import Triton-based RMSNorm: No module named 'liger_kernel'. Falling back to PyTorch implementation.\n",
      "Unable to import Triton-based MLP: No module named 'liger_kernel'. Falling back to vanilla SwiGLU MLP instead.\n",
      "Unable to import Triton-based RMSNorm: No module named 'liger_kernel'. Falling back to PyTorch implementation.\n",
      "Unable to import Triton-based RMSNorm: No module named 'liger_kernel'. Falling back to PyTorch implementation.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "import argparse\n",
    "import json\n",
    "from time import time\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import tiktoken\n",
    "import yaml \n",
    "from safetensors import safe_open\n",
    "import numpy as nps\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tiktoken.load import load_tiktoken_bpe\n",
    "from model import FlashSTU, FlashSTUConfig\n",
    "from flash_stu.utils.stu_utils import get_spectral_filters\n",
    "from flash_stu.utils.random_utils import get_logger, save_yaml_config\n",
    "import math\n",
    "from typing import Union\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8940853-08e2-4860-9d78-cf98813b72e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)\n",
    "bpe_path = \"./o200k_base.tiktoken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fc31159-2af1-4610-8fff-cb6b20b40408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference import (\n",
    "    set_initial_random_seed,\n",
    "    apply_compile,\n",
    "    load_stu_model,\n",
    "    generate_text,\n",
    "    generate_and_time,\n",
    "    main\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f40cd576-0d6c-48c1-92a5-fa41cad042b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_path = \"./configs/baseline_w_attn_bfloat16/eval.yaml\"\n",
    "checkpoint_path = \"./model_step-114000.safetensors\"\n",
    "config_path = \"./configs/lds_bfloat16_w_attn/config.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a01d57c4-1b9b-4dec-8240-3419717a6635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:19:33,147 - __main__ - INFO - For this experiment, saving path is: outputs/april7/543186\n",
      "INFO:__main__:For this experiment, saving path is: outputs/april7/543186\n",
      "2025-04-09 16:19:33,149 - __main__ - INFO - eval_config: {'random_seed': 74, 'run_name': 'baseline_attn_bfloat16', 'save_dir': 'outputs/april7/', 'input_length': [0], 'max_length': [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072], 'temperature': 1, 'top_k': 1, 'repeat': 3, 'cache': True, 'debug': False}\n",
      "INFO:__main__:eval_config: {'random_seed': 74, 'run_name': 'baseline_attn_bfloat16', 'save_dir': 'outputs/april7/', 'input_length': [0], 'max_length': [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072], 'temperature': 1, 'top_k': 1, 'repeat': 3, 'cache': True, 'debug': False}\n",
      "2025-04-09 16:19:33,151 - __main__ - INFO - model_config: {'model_type': 'FlashSTU', 'dim': 896, 'num_heads': 8, 'num_layers': 12, 'seq_len': 8192, 'weight_tying': True, 'window_size': 1024, 'vocab_size': 200064, 'mlp_scale': 12, 'bias': False, 'dropout': 0.1, 'num_eigh': 24, 'use_hankel_L': False, 'num_epochs': 1, 'global_bsz': 524288, 'bsz': 8, 'warmup_steps': 19070, 'eval_period': 50, 'save_period': 19000, 'max_lr': 0.005, 'min_lr': 3e-05, 'max_norm': 1.0, 'dilation': 2, 'fsdp': True, 'ddp': False, 'mixed_precision': True, 'torch_dtype': 'bfloat16', 'cpu_offload': False, 'sharding_strategy': 'full_shard', 'state_dict_type': 'full', 'auto_wrap_policy': 'partial', 'backward_prefetch': 'backward_pre', 'forward_prefetch': False, 'sync_module_states': True, 'use_orig_params': True, 'device_id': None, 'precision': {'param': 'bfloat16', 'reduce': 'bfloat16', 'buffer': 'bfloat16'}, 'fsdp_modules': ['STULayer', 'AttentionLayer'], 'use_activation_checkpointing': True, 'use_flash_fft': True, 'use_approx': True, 'use_attn': True, 'softcap': 50.0, 'theta': 10000.0, 'use_alibi': False, 'torch_compile': False}\n",
      "INFO:__main__:model_config: {'model_type': 'FlashSTU', 'dim': 896, 'num_heads': 8, 'num_layers': 12, 'seq_len': 8192, 'weight_tying': True, 'window_size': 1024, 'vocab_size': 200064, 'mlp_scale': 12, 'bias': False, 'dropout': 0.1, 'num_eigh': 24, 'use_hankel_L': False, 'num_epochs': 1, 'global_bsz': 524288, 'bsz': 8, 'warmup_steps': 19070, 'eval_period': 50, 'save_period': 19000, 'max_lr': 0.005, 'min_lr': 3e-05, 'max_norm': 1.0, 'dilation': 2, 'fsdp': True, 'ddp': False, 'mixed_precision': True, 'torch_dtype': 'bfloat16', 'cpu_offload': False, 'sharding_strategy': 'full_shard', 'state_dict_type': 'full', 'auto_wrap_policy': 'partial', 'backward_prefetch': 'backward_pre', 'forward_prefetch': False, 'sync_module_states': True, 'use_orig_params': True, 'device_id': None, 'precision': {'param': 'bfloat16', 'reduce': 'bfloat16', 'buffer': 'bfloat16'}, 'fsdp_modules': ['STULayer', 'AttentionLayer'], 'use_activation_checkpointing': True, 'use_flash_fft': True, 'use_approx': True, 'use_attn': True, 'softcap': 50.0, 'theta': 10000.0, 'use_alibi': False, 'torch_compile': False}\n"
     ]
    }
   ],
   "source": [
    "eval_config = yaml.load(open(eval_path, 'r'), Loader=yaml.FullLoader)\n",
    "with open(config_path, \"r\") as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Create output dir; save yaml configs in there\n",
    "run_id = random.randint(0, 10 ** 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_initial_random_seed(eval_config.get('random_seed', -1))\n",
    "\n",
    "# save yaml configs in there\n",
    "save_path_for_this_exp = os.path.join(eval_config.get('save_dir'), str(run_id))\n",
    "os.makedirs(save_path_for_this_exp, exist_ok=True)\n",
    "logger.info(f\"For this experiment, saving path is: {save_path_for_this_exp}\")\n",
    "logger.info(f\"eval_config: {eval_config}\")\n",
    "logger.info(f\"model_config: {model_config}\")\n",
    "save_yaml_config(eval_config, save_path_for_this_exp, \"eval_config.yaml\")\n",
    "save_yaml_config(model_config, save_path_for_this_exp, \"model_config.yaml\")\n",
    "\n",
    "# Need to calculate futurefill K here ... TO BE IMPROVED\n",
    "futurefill_k = eval_config.get(\"futurefill_k\", None)\n",
    "if futurefill_k is not None:\n",
    "    if isinstance(futurefill_k[-1], str) and futurefill_k[-1] == \"None\":\n",
    "        generation_L = eval_config.get(\"max_length\")[-1]\n",
    "        futurefill_k = int(math.sqrt(generation_L * math.log2(generation_L)))\n",
    "    elif isinstance(futurefill_k[-1], int):\n",
    "        futurefill_k = futurefill_k[-1]\n",
    "\n",
    "lds_state_dim = eval_config.get('lds_state_dim', None)\n",
    "lds_path = eval_config.get('lds_path', None)\n",
    "\n",
    "# Load model and config.\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a68a3e3-c103-4a5e-8967-11727cb6f33a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameter Count: 550.31M\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ds6237/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1326: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/aten/src/ATen/native/Copy.cpp:308.)\n",
      "  return t.to(\n",
      "2025-04-09 16:19:45,336 - inference - INFO - Loading checkpoint from ./model_step-114000.safetensors...\n",
      "INFO:inference:Loading checkpoint from ./model_step-114000.safetensors...\n",
      "2025-04-09 16:19:45,610 - inference - INFO - Checkpoint loaded in 0.27 seconds.\n",
      "INFO:inference:Checkpoint loaded in 0.27 seconds.\n",
      "2025-04-09 16:19:45,615 - inference - INFO - Model weights loaded successfully!\n",
      "INFO:inference:Model weights loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "model, config_data = load_stu_model(model_config, checkpoint_path, device, futurefill_k = futurefill_k, lds_state_dim = lds_state_dim, lds_path = lds_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a90004a-1fe2-4516-9b6a-b4288f55a087",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer (for della)\n",
    "bpe_dict = load_tiktoken_bpe(bpe_path)\n",
    "tokenizer = tiktoken.Encoding(\n",
    "    name=\"o200k_base\",  # Name of the encoding\n",
    "    pat_str=r\"\"\"'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+\"\"\",\n",
    "    mergeable_ranks=bpe_dict,\n",
    "    special_tokens={\n",
    "        \"<|endoftext|>\": 199999,  # Custom special token example (modify as needed)\n",
    "        \"<|endofprompt|>\": 200018,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a30b8550-a110-46ef-8fe1-6120e071b785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 16:19:49,440 - inference - INFO - Generating text for prompt 1 of length 0. Max generation is 32\n",
      "INFO:inference:Generating text for prompt 1 of length 0. Max generation is 32\n",
      "100%|██████████| 28/28 [00:02<00:00, 11.16it/s]\n",
      "2025-04-09 16:19:52,021 - inference - INFO - Current runtime: 2.516732931137085\n",
      "INFO:inference:Current runtime: 2.516732931137085\n",
      "2025-04-09 16:19:52,022 - inference - INFO - Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "INFO:inference:Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "2025-04-09 16:19:52,024 - inference - INFO - Generating text for prompt 1 of length 0. Max generation is 32\n",
      "INFO:inference:Generating text for prompt 1 of length 0. Max generation is 32\n",
      "100%|██████████| 28/28 [00:02<00:00, 11.91it/s]\n",
      "2025-04-09 16:19:54,431 - inference - INFO - Current runtime: 2.3539979457855225\n",
      "INFO:inference:Current runtime: 2.3539979457855225\n",
      "2025-04-09 16:19:54,433 - inference - INFO - Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "INFO:inference:Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "2025-04-09 16:19:54,435 - inference - INFO - Generating text for prompt 1 of length 0. Max generation is 32\n",
      "INFO:inference:Generating text for prompt 1 of length 0. Max generation is 32\n",
      "100%|██████████| 28/28 [00:02<00:00, 11.91it/s]\n",
      "2025-04-09 16:19:56,854 - inference - INFO - Current runtime: 2.3546950817108154\n",
      "INFO:inference:Current runtime: 2.3546950817108154\n",
      "2025-04-09 16:19:56,856 - inference - INFO - Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "INFO:inference:Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "2025-04-09 16:19:56,858 - inference - INFO - final_length: 32\n",
      "INFO:inference:final_length: 32\n",
      "2025-04-09 16:19:56,860 - inference - INFO - Mean runtime: 2.408475319544474\n",
      "INFO:inference:Mean runtime: 2.408475319544474\n",
      "2025-04-09 16:19:56,861 - inference - INFO - Mean runtime wo first: 2.354346513748169\n",
      "INFO:inference:Mean runtime wo first: 2.354346513748169\n",
      "2025-04-09 16:19:56,863 - inference - INFO - std runtimes: 0.07655022033683508\n",
      "INFO:inference:std runtimes: 0.07655022033683508\n",
      "2025-04-09 16:19:56,865 - inference - INFO - std runtimes wo first: 0.0\n",
      "INFO:inference:std runtimes wo first: 0.0\n",
      "2025-04-09 16:19:56,866 - inference - INFO - min runtimes: 2.3539979457855225\n",
      "INFO:inference:min runtimes: 2.3539979457855225\n",
      "2025-04-09 16:19:56,868 - inference - INFO - max runtimes: 2.516732931137085\n",
      "INFO:inference:max runtimes: 2.516732931137085\n",
      "2025-04-09 16:19:56,869 - inference - INFO - -----\n",
      "\n",
      "INFO:inference:-----\n",
      "\n",
      "2025-04-09 16:19:56,871 - inference - INFO - Generating text for prompt 1 of length 0. Max generation is 64\n",
      "INFO:inference:Generating text for prompt 1 of length 0. Max generation is 64\n",
      "100%|██████████| 60/60 [00:05<00:00, 11.89it/s]\n",
      "2025-04-09 16:20:01,976 - inference - INFO - Current runtime: 5.048954963684082\n",
      "INFO:inference:Current runtime: 5.048954963684082\n",
      "2025-04-09 16:20:01,978 - inference - INFO - Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "INFO:inference:Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "2025-04-09 16:20:01,979 - inference - INFO - Generating text for prompt 1 of length 0. Max generation is 64\n",
      "INFO:inference:Generating text for prompt 1 of length 0. Max generation is 64\n",
      "100%|██████████| 60/60 [00:05<00:00, 11.92it/s]\n",
      "2025-04-09 16:20:07,070 - inference - INFO - Current runtime: 5.0369908809661865\n",
      "INFO:inference:Current runtime: 5.0369908809661865\n",
      "2025-04-09 16:20:07,072 - inference - INFO - Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "INFO:inference:Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "2025-04-09 16:20:07,074 - inference - INFO - Generating text for prompt 1 of length 0. Max generation is 64\n",
      "INFO:inference:Generating text for prompt 1 of length 0. Max generation is 64\n",
      "100%|██████████| 60/60 [00:05<00:00, 11.92it/s]\n",
      "2025-04-09 16:20:12,168 - inference - INFO - Current runtime: 5.038620233535767\n",
      "INFO:inference:Current runtime: 5.038620233535767\n",
      "2025-04-09 16:20:12,170 - inference - INFO - Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "INFO:inference:Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "2025-04-09 16:20:12,172 - inference - INFO - final_length: 64\n",
      "INFO:inference:final_length: 64\n",
      "2025-04-09 16:20:12,173 - inference - INFO - Mean runtime: 5.041522026062012\n",
      "INFO:inference:Mean runtime: 5.041522026062012\n",
      "2025-04-09 16:20:12,175 - inference - INFO - Mean runtime wo first: 5.037805557250977\n",
      "INFO:inference:Mean runtime wo first: 5.037805557250977\n",
      "2025-04-09 16:20:12,176 - inference - INFO - std runtimes: 0.005297805754535897\n",
      "INFO:inference:std runtimes: 0.005297805754535897\n",
      "2025-04-09 16:20:12,178 - inference - INFO - std runtimes wo first: 0.0\n",
      "INFO:inference:std runtimes wo first: 0.0\n",
      "2025-04-09 16:20:12,180 - inference - INFO - min runtimes: 5.0369908809661865\n",
      "INFO:inference:min runtimes: 5.0369908809661865\n",
      "2025-04-09 16:20:12,181 - inference - INFO - max runtimes: 5.048954963684082\n",
      "INFO:inference:max runtimes: 5.048954963684082\n",
      "2025-04-09 16:20:12,183 - inference - INFO - -----\n",
      "\n",
      "INFO:inference:-----\n",
      "\n",
      "2025-04-09 16:20:12,184 - inference - INFO - Generating text for prompt 1 of length 0. Max generation is 128\n",
      "INFO:inference:Generating text for prompt 1 of length 0. Max generation is 128\n",
      "100%|██████████| 124/124 [00:10<00:00, 11.91it/s]\n",
      "2025-04-09 16:20:22,652 - inference - INFO - Current runtime: 10.410698652267456\n",
      "INFO:inference:Current runtime: 10.410698652267456\n",
      "2025-04-09 16:20:22,654 - inference - INFO - Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "INFO:inference:Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "2025-04-09 16:20:22,656 - inference - INFO - Generating text for prompt 1 of length 0. Max generation is 128\n",
      "INFO:inference:Generating text for prompt 1 of length 0. Max generation is 128\n",
      "100%|██████████| 124/124 [00:10<00:00, 11.91it/s]\n",
      "2025-04-09 16:20:33,128 - inference - INFO - Current runtime: 10.41715693473816\n",
      "INFO:inference:Current runtime: 10.41715693473816\n",
      "2025-04-09 16:20:33,129 - inference - INFO - Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "INFO:inference:Output: ['Obama is famous for CMUN \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook \"cook']\n",
      "2025-04-09 16:20:33,131 - inference - INFO - Generating text for prompt 1 of length 0. Max generation is 128\n",
      "INFO:inference:Generating text for prompt 1 of length 0. Max generation is 128\n",
      " 68%|██████▊   | 84/124 [00:07<00:03, 11.78it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m generate_and_time(model, tokenizer, eval_config, save_path_for_this_exp, device)\n",
      "File \u001b[0;32m~/DeepLDS/inference.py:233\u001b[0m, in \u001b[0;36mgenerate_and_time\u001b[0;34m(model, tokenizer, eval_config, save_path_for_this_exp, device)\u001b[0m\n\u001b[1;32m    230\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObama is famous for\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    232\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[0;32m--> 233\u001b[0m tokens \u001b[38;5;241m=\u001b[39m generate_text(\n\u001b[1;32m    234\u001b[0m     model,\n\u001b[1;32m    235\u001b[0m     tokenizer,\n\u001b[1;32m    236\u001b[0m     prompt, \u001b[38;5;66;03m#_dummy_prompt_ids,\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    238\u001b[0m     max_length\u001b[38;5;241m=\u001b[39mfinal_length,\n\u001b[1;32m    239\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m    240\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mBASE_TEMPERATURE,\n\u001b[1;32m    241\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mBASE_TOP_K,\n\u001b[1;32m    242\u001b[0m     cache \u001b[38;5;241m=\u001b[39m cache, \n\u001b[1;32m    243\u001b[0m     futurefill_k \u001b[38;5;241m=\u001b[39m futurefill_k\n\u001b[1;32m    244\u001b[0m )\n\u001b[1;32m    246\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time()\n\u001b[1;32m    247\u001b[0m current_runtime \u001b[38;5;241m=\u001b[39m end_time \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/DeepLDS/inference.py:167\u001b[0m, in \u001b[0;36mgenerate_text\u001b[0;34m(model, tokenizer, prompt, num_return_sequences, max_length, device, temperature, top_k, cache, futurefill_k)\u001b[0m\n\u001b[1;32m    165\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(tokens, input_pos \u001b[38;5;241m=\u001b[39m input_pos)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m idx \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 167\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(tokens[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:], input_pos \u001b[38;5;241m=\u001b[39m input_pos)     \u001b[38;5;66;03m# shape: [batch, 1, vocab]\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(tokens, input_pos \u001b[38;5;241m=\u001b[39m input_pos)     \u001b[38;5;66;03m# shape: [batch, seq, vocab]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/DeepLDS/model.py:165\u001b[0m, in \u001b[0;36mFlashSTU.forward\u001b[0;34m(self, x, input_pos, *kargs, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(layer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn\u001b[39m\u001b[38;5;124m\"\u001b[39m): \u001b[38;5;66;03m# Pass RoPE freq_cis to attention layers\u001b[39;00m\n\u001b[0;32m--> 165\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, freqs_cis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfreqs_cis, input_pos\u001b[38;5;241m=\u001b[39minput_pos, \u001b[38;5;241m*\u001b[39mkargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    167\u001b[0m         x \u001b[38;5;241m=\u001b[39m layer(x, input_pos \u001b[38;5;241m=\u001b[39m input_pos, \u001b[38;5;241m*\u001b[39mkargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/DeepLDS/flash_stu/layers/attention_layer.py:42\u001b[0m, in \u001b[0;36mAttentionLayer.forward\u001b[0;34m(self, x, freqs_cis, input_pos)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, freqs_cis: torch\u001b[38;5;241m.\u001b[39mTensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, input_pos: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m---> 42\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn_norm(x), freqs_cis\u001b[38;5;241m=\u001b[39mfreqs_cis, input_pos\u001b[38;5;241m=\u001b[39minput_pos)\n\u001b[1;32m     43\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp_norm(x))\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/DeepLDS/flash_stu/modules/attention.py:99\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, q, k, v, freqs_cis, input_pos)\u001b[0m\n\u001b[1;32m     96\u001b[0m         k \u001b[38;5;241m=\u001b[39m k[:, :\u001b[38;5;28mmax\u001b[39m(input_pos)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     97\u001b[0m         v \u001b[38;5;241m=\u001b[39m v[:, :\u001b[38;5;28mmax\u001b[39m(input_pos)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 99\u001b[0m y \u001b[38;5;241m=\u001b[39m flash_attn_func(\n\u001b[1;32m    100\u001b[0m     q\u001b[38;5;241m=\u001b[39mq, k\u001b[38;5;241m=\u001b[39mk, v\u001b[38;5;241m=\u001b[39mv,\n\u001b[1;32m    101\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    102\u001b[0m     causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    103\u001b[0m     window_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwindow_size, \u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    104\u001b[0m     alibi_slopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malibi_slopes,\n\u001b[1;32m    105\u001b[0m )\n\u001b[1;32m    107\u001b[0m y \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz, q_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    109\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresid_dropout(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_proj(y))\n",
      "File \u001b[0;32m~/.conda/envs/torch-env/lib/python3.12/site-packages/flash_attn/flash_attn_interface.py:1140\u001b[0m, in \u001b[0;36mflash_attn_func\u001b[0;34m(q, k, v, dropout_p, softmax_scale, causal, window_size, softcap, alibi_slopes, deterministic, return_attn_probs)\u001b[0m\n\u001b[1;32m   1074\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"dropout_p should be set to 0.0 during evaluation\u001b[39;00m\n\u001b[1;32m   1075\u001b[0m \u001b[38;5;124;03m    If K, V are already stacked into 1 tensor, this function will be faster than\u001b[39;00m\n\u001b[1;32m   1076\u001b[0m \u001b[38;5;124;03m    calling flash_attn_func on Q, K, V since the backward pass avoids explicit concatenation\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;124;03m            pattern (negative means that location was dropped, nonnegative means it was kept).\u001b[39;00m\n\u001b[1;32m   1124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FlashAttnKVPackedFunc\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m   1126\u001b[0m         q,\n\u001b[1;32m   1127\u001b[0m         kv,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1136\u001b[0m         torch\u001b[38;5;241m.\u001b[39mis_grad_enabled(),\n\u001b[1;32m   1137\u001b[0m     )\n\u001b[0;32m-> 1140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mflash_attn_func\u001b[39m(\n\u001b[1;32m   1141\u001b[0m     q,\n\u001b[1;32m   1142\u001b[0m     k,\n\u001b[1;32m   1143\u001b[0m     v,\n\u001b[1;32m   1144\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m   1145\u001b[0m     softmax_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1146\u001b[0m     causal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1147\u001b[0m     window_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),  \u001b[38;5;66;03m# -1 means infinite context window\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m     softcap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;66;03m# 0.0 means deactivated\u001b[39;00m\n\u001b[1;32m   1149\u001b[0m     alibi_slopes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1150\u001b[0m     deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1151\u001b[0m     return_attn_probs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1152\u001b[0m ):\n\u001b[1;32m   1153\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"dropout_p should be set to 0.0 during evaluation\u001b[39;00m\n\u001b[1;32m   1154\u001b[0m \u001b[38;5;124;03m    Supports multi-query and grouped-query attention (MQA/GQA) by passing in KV with fewer heads\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m \u001b[38;5;124;03m    than Q. Note that the number of heads in Q must be divisible by the number of heads in KV.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1199\u001b[0m \u001b[38;5;124;03m            pattern (negative means that location was dropped, nonnegative means it was kept).\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m FlashAttnFunc\u001b[38;5;241m.\u001b[39mapply(\n\u001b[1;32m   1202\u001b[0m         q,\n\u001b[1;32m   1203\u001b[0m         k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1213\u001b[0m         torch\u001b[38;5;241m.\u001b[39mis_grad_enabled(),\n\u001b[1;32m   1214\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "generate_and_time(model, tokenizer, eval_config, save_path_for_this_exp, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaba33f9-04ed-4851-8073-176eb1091471",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-env [~/.conda/envs/torch-env/]",
   "language": "python",
   "name": "conda_torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
